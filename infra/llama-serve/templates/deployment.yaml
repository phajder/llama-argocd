apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "llama-serve.fullname" . }}
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      {{- include "llama-serve.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "llama-serve.labels" . | nindent 8 }}
        {{- with .Values.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      {{ if .Values.gpu.enabled }}
      runtimeClassName: nvidia
      {{ end }}
      containers:
        - name: {{ .Chart.Name }}
          image: {{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command: [ "python3", "-m", "llama_cpp.server" ]
          ports:
          - name: web
            containerPort: {{ .Values.service.port }}
          {{ if .Values.gpu.enabled }}
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          {{ end }}
          volumeMounts:
            - name: models
              mountPath: {{ .Values.env.mountPath }}
              readOnly: true
          env:
            {{ if .Values.gpu.enabled }}
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: all
            - name: N_GPU_LAYERS
              value: "{{ .Values.gpu.gpuLayers }}"
            {{ end }}
            - name: HOST
              value: 0.0.0.0
            - name: PORT
              value: "{{ .Values.service.port }}"
            - name: MODEL
              value: "{{ .Values.env.mountPath }}/{{ .Values.env.modelFile }}"
            - name: CHAT_FORMAT
              value: {{ .Values.env.chatFormat }}
      volumes:
        - name: models
          hostPath:
            path: {{ .Values.env.modelHostPath }}
